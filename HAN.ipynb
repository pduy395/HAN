{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "id": "lUwKcrCwpaaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e383dd90-7c66-49fe-e4f1-22d9d17e9923"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.11 / client 1.5.16)\n",
            "ref                                                         title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "sudarshan24byte/online-food-dataset                         Online Food Dataset                              3KB  2024-03-02 18:50:30          22780        454  0.9411765        \n",
            "alistairking/electricity-prices                             U.S. Electricity Prices                          1MB  2024-04-07 19:18:37            688         24  1.0              \n",
            "fatemehmehrparvar/obesity-levels                            Obesity Levels                                  58KB  2024-04-07 16:28:30            790         24  0.88235295       \n",
            "sukhmandeepsinghbrar/housing-price-dataset                  Housing Price Dataset                          780KB  2024-04-04 19:45:43           1150         24  1.0              \n",
            "bhavikjikadara/student-study-performance                    Student Study Performance                        9KB  2024-03-07 06:14:09          11281        151  1.0              \n",
            "nbroad/gemma-rewrite-nbroad                                 gemma-rewrite-nbroad                             8MB  2024-03-03 04:52:39           1454         94  1.0              \n",
            "mohdshahnawazaadil/supermarket-superstore-dataset-bundle    Supermarket / Superstore Dataset Bundle        198MB  2024-04-02 10:25:15            757         28  1.0              \n",
            "sahirmaharajj/employee-salaries-analysis                    Employee Salaries Analysis                     101KB  2024-03-31 16:32:47            609         24  1.0              \n",
            "sahirmaharajj/retail-sales-analysis                         Retail Sales Analysis                            6MB  2024-03-31 15:37:11           1444         33  1.0              \n",
            "sahirmaharajj/air-pollution-dataset                         Air Pollution Dataset                          213KB  2024-04-07 13:14:48            803         27  1.0              \n",
            "jatinthakur706/most-watched-netflix-original-shows-tv-time  Most watched Netflix original shows (TV Time)    2KB  2024-03-27 09:01:21           2565         45  1.0              \n",
            "bhavyadhingra00020/india-rental-house-price                 Indian Rental House Price                      849KB  2024-04-07 11:12:35            395         24  1.0              \n",
            "sahirmaharajj/electric-vehicle-population-size-2024         Electric Vehicle Population by Country (2024)  275KB  2024-03-30 19:16:06           1808         46  1.0              \n",
            "mexwell/drug-consumption-classification                     ðŸ’Š Drug Consumption Classification               56KB  2024-03-28 15:02:22           2125         41  1.0              \n",
            "shriyashjagtap/fraudulent-e-commerce-transactions           ðŸš¨ Fraudulent E-Commerce Transactions ðŸ’³         159MB  2024-04-07 03:30:07            487         23  1.0              \n",
            "mohdshahnawazaadil/restaurant-dataset                       Restaurant Dataset                             433KB  2024-04-03 21:42:58            965         25  1.0              \n",
            "saurabhbadole/walmart-super-market-dataset                  Walmart - Super Market Dataset                   4MB  2024-03-28 19:31:48           1433         24  0.9411765        \n",
            "syedanwarafridi/vehicle-sales-data                          Vehicle Sales Data                              19MB  2024-02-21 20:16:17          26481        403  1.0              \n",
            "datascientist97/e-commerece-sales-data-2024                 E-commerece Sales Data 2024                      6MB  2024-04-05 12:32:58            760         26  1.0              \n",
            "lovishbansal123/sales-of-a-supermarket                      Sales of a Supermarket                          36KB  2024-03-26 12:38:06           3084         53  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "4osY4bIGqvxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8284d30-cd15-40a8-8ebd-8e5dfa656b80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf ag_news_csv.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1FeBkuqAnC",
        "outputId": "59f45b12-505b-451e-9dc8-96b69f56456f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ag_news_csv/\n",
            "ag_news_csv/train.csv\n",
            "ag_news_csv/test.csv\n",
            "ag_news_csv/classes.txt\n",
            "ag_news_csv/readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d 'rtatman/glove-global-vectors-for-word-representation'\n",
        "!unzip glove-global-vectors-for-word-representation.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iocvey6rqX2K",
        "outputId": "e083cbc9-645e-4b3a-9a92-10f78c697e34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glove-global-vectors-for-word-representation.zip to /content\n",
            "100% 458M/458M [00:18<00:00, 31.4MB/s]\n",
            "100% 458M/458M [00:18<00:00, 26.4MB/s]\n",
            "Archive:  glove-global-vectors-for-word-representation.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data process"
      ],
      "metadata": {
        "id": "neD87Uy_VYFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import vocab\n",
        "glove = vocab.Vectors('glove.6B.100d.txt', 'train/')\n",
        "\n",
        "print(f'Shape of GloVe vectors is {glove.vectors.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnN2Z0GMZgx7",
        "outputId": "c2faf3fe-d3f7-43d3-b583-8ba7d51ec1b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 399999/400000 [00:22<00:00, 17718.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of GloVe vectors is torch.Size([400000, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove['a'].numpy())"
      ],
      "metadata": {
        "id": "RW5Tk5SBdEuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1pV0xTL4pIUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c0ae12-0f66-45f4-abf1-61bab25c4ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path, max_length_sentences=30, max_length_word=35, transform=None, target_transform=None):\n",
        "        super(MyDataset, self).__init__()\n",
        "\n",
        "        texts, labels = [], []\n",
        "        with open(data_path) as csv_file:\n",
        "            reader = csv.reader(csv_file, quotechar='\"')\n",
        "            for idx, line in enumerate(reader):\n",
        "                text = \"\"\n",
        "                for tx in line[1:]:\n",
        "                    text += tx.lower()\n",
        "                    text += \". \"\n",
        "                label = int(line[0])\n",
        "                texts.append(text)\n",
        "                labels.append(label)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "        self.max_length_sentences = max_length_sentences\n",
        "        self.max_length_word = max_length_word\n",
        "        self.num_classes = len(set(self.labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        text = self.texts[index]\n",
        "        document_encode = [\n",
        "            [glove[word].numpy() for word in word_tokenize(text=sentences)] for sentences\n",
        "            in\n",
        "            sent_tokenize(text=text)]\n",
        "\n",
        "        for sentences in document_encode:\n",
        "            if len(sentences) < self.max_length_word:\n",
        "                extended_words = [glove['<pad>'].numpy() for _ in range(self.max_length_word - len(sentences))]\n",
        "                sentences.extend(extended_words)\n",
        "\n",
        "        if len(document_encode) < self.max_length_sentences:\n",
        "            extended_sentences = [[glove['<pad>'].numpy() for _ in range(self.max_length_word)] for _ in\n",
        "                                  range(self.max_length_sentences - len(document_encode))]\n",
        "            document_encode.extend(extended_sentences)\n",
        "\n",
        "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
        "                          :self.max_length_sentences]\n",
        "\n",
        "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
        "\n",
        "        return torch.from_numpy(document_encode), F.one_hot(torch.tensor(label-1), num_classes=4).to(torch.float)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "    print (test.__getitem__(index=0)[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"eminem is represented by the index location at: {glove.stoi['pad']} and has the following vector values: \\n {glove['eminem']}\")\n",
        "print(glove['<pad>'])"
      ],
      "metadata": {
        "id": "lZWxphQYa3Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584399fd-e4fa-46f3-a602-79fb793ce808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eminem is represented by the index location at: 10109 and has the following vector values: \n",
            " tensor([ 0.7544, -0.0373,  0.9011, -0.2475,  0.6046,  0.2633,  0.4321, -0.0581,\n",
            "         0.2454,  0.4725,  0.4708,  0.6857, -0.1124, -0.5863,  0.6069,  0.2199,\n",
            "         0.3303, -0.4111,  0.4995,  0.5576,  0.5199, -0.7013,  0.1960,  0.0222,\n",
            "         0.1784,  1.2870,  0.0808, -0.0457,  0.6343, -0.1123,  0.5205,  0.5357,\n",
            "         0.6573,  1.4612, -0.5139,  0.1027,  0.3214,  0.3201, -0.2493, -0.1228,\n",
            "         0.7139,  0.5747, -0.0808,  0.0766, -0.8888, -0.6424, -0.3580, -0.1718,\n",
            "         0.0162, -0.4686, -0.1671, -0.2047,  0.3036,  0.0647, -0.4121, -0.6413,\n",
            "         0.0488, -0.1874, -1.0129,  0.3502, -0.0286,  0.8700, -0.1154,  0.0711,\n",
            "         0.8362,  0.0659,  1.2272,  0.4069,  0.0153,  0.0362, -0.3664,  0.8553,\n",
            "        -0.8979,  0.5335, -0.2803, -0.4116,  0.1657, -0.0971,  0.8507, -1.0010,\n",
            "         0.6556, -0.2359,  0.1473,  0.4138, -0.9671,  0.0302, -0.4406, -0.0727,\n",
            "         0.1327,  0.0797, -0.1519,  0.2030, -0.6210, -0.7379,  0.4092, -0.1602,\n",
            "        -0.1799,  0.2000,  0.3661,  0.1309])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_set = MyDataset(data_path=\"ag_news_csv/train.csv\")\n",
        "\n",
        "\n",
        "# class_weights = [0.25, 0.25, 0.25, 0.25]\n",
        "# sampler = WeightedRandomSampler(weights=class_weights, num_samples=len(train_set), replacement=True)\n",
        "# dataloader = DataLoader(train_set, batch_size=32, sampler=sampler)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(train_set, batch_size=32,shuffle=True)\n",
        "\n",
        "\n",
        "val_set = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "val_dataloader = DataLoader(val_set, batch_size=32,shuffle=True)\n",
        "\n",
        "# test_set = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "# test_dataloader = DataLoader(test_set, batch_size=32,shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5qPX6sYbOQq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build model"
      ],
      "metadata": {
        "id": "-1rLE8LeVWfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ni8uV70GWm27"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval==0.0.12"
      ],
      "metadata": {
        "id": "xucVH6wytYC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff92e7d9-1168-43a2-e076-7c11732ee031"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval==0.0.12\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval==0.0.12) (1.25.2)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.10/dist-packages (from seqeval==0.0.12) (2.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7415 sha256=d1be1a71b79ec6ca22fe573f2fabe5508379f082c6b0d1e107ed1d7f1824a50c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/6c/fc/7076d687ba54f32c7be7eaaded97df359ef3c8fee08a2d4efc\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def get_acc(network,val_test_iter,device):\n",
        "    network.eval() # set the model in evaluation mode to not compute gradients and reduce overhead\n",
        "    with torch.no_grad(): # turn of gradients calculation\n",
        "        preds =[]\n",
        "        labels=[]\n",
        "        for batch in val_test_iter:\n",
        "            batch =[t.to(device) for t in batch]\n",
        "\n",
        "            if len(batch[1]) < 32:\n",
        "              network._init_hidden_state(last_batch_size=len(batch[1]))\n",
        "            else:\n",
        "              network._init_hidden_state()\n",
        "            pred = network(batch[0])\n",
        "            preds.append(pred)\n",
        "            labels.append(batch[1])\n",
        "\n",
        "        preds = torch.cat(preds, dim=0)\n",
        "        labels = torch.cat(labels, dim=0)\n",
        "        assert len(preds) == len(labels)\n",
        "        return preds, labels\n",
        "        return {\n",
        "            \"acc_precision\": precision_score(labels, preds),\n",
        "            \"acc_recall\": recall_score(labels, preds),\n",
        "            \"acc_f1\": f1_score(labels, preds)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_max_lengths(data_path):\n",
        "    word_length_list = []\n",
        "    sent_length_list = []\n",
        "    with open(data_path) as csv_file:\n",
        "        reader = csv.reader(csv_file, quotechar='\"')\n",
        "        for idx, line in enumerate(reader):\n",
        "            text = \"\"\n",
        "            for tx in line[1:]:\n",
        "                text += tx.lower()\n",
        "                text += \" \"\n",
        "            sent_list = sent_tokenize(text)\n",
        "            sent_length_list.append(len(sent_list))\n",
        "\n",
        "            for sent in sent_list:\n",
        "                word_list = word_tokenize(sent)\n",
        "                word_length_list.append(len(word_list))\n",
        "\n",
        "        sorted_word_length = sorted(word_length_list)\n",
        "        sorted_sent_length = sorted(sent_length_list)\n",
        "\n",
        "    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]"
      ],
      "metadata": {
        "id": "tsLj8w0evAG8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierAttNet(nn.Module):\n",
        "    def __init__(self, word_hidden_size=50, sent_hidden_size=50, batch_size=32, num_classes=4,\n",
        "                 max_sent_length=30, max_word_length=35):\n",
        "        super(HierAttNet, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.word_hidden_size = word_hidden_size\n",
        "        self.sent_hidden_size = sent_hidden_size\n",
        "        self.max_sent_length = max_sent_length\n",
        "        self.max_word_length = max_word_length\n",
        "        self.word_att_net = WordAttNet(word_hidden_size)\n",
        "        self.sent_att_net = SentAttNet(sent_hidden_size)\n",
        "        self._init_hidden_state()\n",
        "\n",
        "        self.dense = torch.nn.Linear(sent_hidden_size*2,num_classes,bias=True)\n",
        "\n",
        "    def _init_hidden_state(self, last_batch_size=None):\n",
        "\n",
        "        if last_batch_size:\n",
        "            batch_size = last_batch_size\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
        "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
        "        if torch.cuda.is_available():\n",
        "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
        "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output_list = []\n",
        "\n",
        "        input = input.permute(1, 0, 2, 3)\n",
        "        for i in input: # vá»›i má»—i cÃ¢u i\n",
        "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0, 2), self.word_hidden_state)\n",
        "\n",
        "            output_list.append(output)\n",
        "\n",
        "        #output = torch.cat(output_list, 0)\n",
        "\n",
        "        output = torch.stack(output_list, dim=0)\n",
        "\n",
        "\n",
        "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
        "        #32 100\n",
        "\n",
        "        output= self.dense(output)\n",
        "        output=F.softmax(output,dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "eN5cJEHlVeb7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentAttNet(nn.Module):\n",
        "    def __init__(self,  hidden_size=50):\n",
        "        super(SentAttNet, self).__init__()\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 10))\n",
        "        self.sent_bias = nn.Parameter(torch.Tensor(10))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(10, 1))\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(100, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "\n",
        "        f_output, h_output = self.gru(input, hidden_state)  # feature output and hidden state output\n",
        "        #32 30 100\n",
        "\n",
        "        output = matrix_mul(f_output, self.sent_weight,  self.context_weight, self.sent_bias)\n",
        "        output = F.softmax(output) # 30 32\n",
        "\n",
        "        # 30 32 1 * 30 32 100\n",
        "        output = element_wise_mul(output.permute(1,0,2),f_output.permute(1,0,2))\n",
        "\n",
        "        return output, h_output\n"
      ],
      "metadata": {
        "id": "dW5xbFfTWxVE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class WordAttNet(nn.Module):\n",
        "    def __init__(self,  hidden_size=50):\n",
        "        super(WordAttNet, self).__init__()\n",
        "\n",
        "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 10))\n",
        "        self.word_bias = nn.Parameter(torch.Tensor(10))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(10, 1))\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(100, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "\n",
        "        self.word_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "\n",
        "        f_output, h_output = self.gru(input, hidden_state)  # feature output and hidden state output\n",
        "        #32 35 100\n",
        "\n",
        "        output = matrix_mul(f_output, self.word_weight,  self.context_weight, self.word_bias)\n",
        "        output = F.softmax(output) # 35 32\n",
        "\n",
        "        # 35 32 1 * 35 32 100\n",
        "        output = element_wise_mul(output.permute(1,0,2),f_output.permute(1,0,2))\n",
        "\n",
        "        return output, h_output\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(input1, input2):\n",
        "        #feature_1 = feature_1.squeeze(dim=2)\n",
        "        feature = (feature_1 * feature_2).sum(dim=0)\n",
        "        #print(feature.shape)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    output = torch.stack(feature_list, dim=0)\n",
        "    return output\n",
        "\n",
        "\n",
        "def matrix_mul(input, weight, context_weight,  bias=False):\n",
        "\n",
        "\n",
        "    #feature = feature.unsqueeze(1)\n",
        "    input = torch.matmul(input, weight)\n",
        "    input = input + bias\n",
        "    input = torch.tanh(input)\n",
        "\n",
        "    input = torch.matmul(input, context_weight)\n",
        "\n",
        "    return input"
      ],
      "metadata": {
        "id": "ddZvJUzXWtMr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HierAttNet()\n",
        "model"
      ],
      "metadata": {
        "id": "CN3i07Dcmezb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db35edf1-f42c-4733-a6f1-77e1cc73b711"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HierAttNet(\n",
              "  (word_att_net): WordAttNet(\n",
              "    (gru): GRU(100, 50, bidirectional=True)\n",
              "  )\n",
              "  (sent_att_net): SentAttNet(\n",
              "    (gru): GRU(100, 50, bidirectional=True)\n",
              "  )\n",
              "  (dense): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "ZwrD2FWxi9Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "def train_network(network,train_iter,optimizer,loss_fn,epoch_num,device):\n",
        "\n",
        "    epoch_loss = 0 # loss per epoch\n",
        "    epoch_acc = 0 # accuracy per epoch\n",
        "    network.train() # set the model in training mode as it requires gradients calculation and updtion\n",
        "\n",
        "    network.zero_grad() # clear all the calculated grdients from previous step\n",
        "\n",
        "\n",
        "    # turn off while testing using  model.eval() and torch.no_grad() block\n",
        "\n",
        "    for batch in tqdm(train_iter,f\"Epoch: {epoch_num}\"):\n",
        "        batch =[t.to(device) for t in batch]\n",
        "        # data will be shown to model in batches per epoch to calculate gradients per batch\n",
        "        if len(batch[1]) < 32:\n",
        "              network._init_hidden_state(last_batch_size=len(batch[1]))\n",
        "        else:\n",
        "              network._init_hidden_state()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = network(batch[0])\n",
        "\n",
        "        loss = loss_fn(predictions, batch[1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        pred_classes = torch.zeros_like(predictions)\n",
        "        pred_classes[torch.arange(len(predictions)), torch.argmax(predictions, dim=1)] = 1\n",
        "\n",
        "        correct_preds = (pred_classes * batch[1]).float()\n",
        "\n",
        "        accuracy = correct_preds.sum()/len(correct_preds)\n",
        "\n",
        "\n",
        "        epoch_loss += loss.item()  # add the loss for this batch to calculate the loss for whole epoch\n",
        "        epoch_acc += accuracy.item() # .item() tend to give the exact number from the tensor of shape [1,]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        time.sleep(0.001) # for tqdm progess bar\n",
        "\n",
        "    return epoch_loss/len(train_iter), epoch_acc/len(train_iter)"
      ],
      "metadata": {
        "id": "wAZkni3FjBKc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_network(network,val_test_iter,optimizer,loss_fn,device):\n",
        "    '''\n",
        "    evaluate the network using given parameters\n",
        "    args:\n",
        "        network: any Neural Network object\n",
        "        val_test_iter: iterator of validation/test data\n",
        "        optimizer: optimizer for gradients calculation and updation\n",
        "        loss_fn: appropriate loss function\n",
        "    out:\n",
        "        a tuple of (average_loss,average_accuracy) of floating values for the incoming dataset\n",
        "    '''\n",
        "\n",
        "    total_loss = 0  # total loss for the whole incoming data\n",
        "    total_acc = 0 # total accuracy for the whole data\n",
        "\n",
        "    network.eval() # set the model in evaluation mode to not compute gradients and reduce overhead\n",
        "\n",
        "    with torch.no_grad(): # turn of gradients calculation\n",
        "\n",
        "        for batch in val_test_iter:\n",
        "            batch =[t.to(device) for t in batch]\n",
        "\n",
        "            if len(batch[1]) < 32:\n",
        "              network._init_hidden_state(last_batch_size=len(batch[1]))\n",
        "            else:\n",
        "              network._init_hidden_state()\n",
        "\n",
        "            predictions = network(batch[0])\n",
        "\n",
        "\n",
        "            loss = loss_fn(predictions,batch[1])\n",
        "\n",
        "            pred_classes = torch.zeros_like(predictions)\n",
        "            pred_classes[torch.arange(len(predictions)), torch.argmax(predictions, dim=1)] = 1\n",
        "\n",
        "            correct_preds = (pred_classes * batch[1]).float()\n",
        "\n",
        "            accuracy = correct_preds.sum()/len(correct_preds)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += accuracy.item()\n",
        "\n",
        "        return total_loss/len(val_test_iter), total_acc/len(val_test_iter)"
      ],
      "metadata": {
        "id": "khx4jqJYjjoE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "GG2hmkXSjuhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "network = HierAttNet()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "lr = 3e-4\n",
        "optimizer = torch.optim.Adam(network.parameters(),lr=lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "network.to(device)\n",
        "\n",
        "# optimizer and losses remains the same\n",
        "\n",
        "EPOCH = 3\n",
        "for epoch in range(EPOCH):\n",
        "    train_loss, train_acc = train_network(network,dataloader,optimizer,loss_fn,epoch+1,device)\n",
        "    val_loss,val_acc = evaluate_network(network,val_dataloader,optimizer,loss_fn,device)\n",
        "    tqdm.write(f'''End of Epoch: {epoch+1}  |  Train Loss: {train_loss:.3f}  |  Val Loss: {val_loss:.3f}  |  Train Acc: {train_acc*100:.2f}%  |  Val Acc: {val_acc*100:.2f}%''')\n",
        "\n",
        "\n",
        "torch.save(network.state_dict(), 'model.pth')\n"
      ],
      "metadata": {
        "id": "8urcHRKRjxFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b4e712-bc4f-4a1b-fbf4-2fa723cd7bf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch: 1:   0%|          | 0/3750 [00:00<?, ?it/s]<ipython-input-11-89cfe766dfeb>:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(output) # 35 32\n",
            "Epoch: 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [27:21<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch: 1  |  Train Loss: 0.897  |  Val Loss: 0.854  |  Train Acc: 85.02%  |  Val Acc: 88.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [27:12<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch: 2  |  Train Loss: 0.843  |  Val Loss: 0.842  |  Train Acc: 89.93%  |  Val Acc: 90.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [27:29<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch: 3  |  Train Loss: 0.837  |  Val Loss: 0.841  |  Train Acc: 90.51%  |  Val Acc: 90.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = HierAttNet()\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "lr = 3e-4\n",
        "optimizer = torch.optim.Adam(network.parameters(),lr=lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "val_loss,val_acc = evaluate_network(model,val_dataloader,optimizer,loss_fn,device)\n",
        "tqdm.write(f'''End:  |  Test Loss: {val_loss:.3f}  |   Test Acc: {val_acc*100:.2f}%''')\n"
      ],
      "metadata": {
        "id": "WC8WmJBjoZcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6203fc06-76f8-470d-9848-c236ee75f4d2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-89cfe766dfeb>:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(output) # 35 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End:  |  Test Loss: 0.841  |   Test Acc: 90.13%\n"
          ]
        }
      ]
    }
  ]
}