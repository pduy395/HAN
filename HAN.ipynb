{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUwKcrCwpaaa"
      },
      "outputs": [],
      "source": [
        "\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4osY4bIGqvxF",
        "outputId": "e628ca19-d6eb-4d96-d675-26316a27bd61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WwIhW9PIpyAE"
      },
      "outputs": [],
      "source": [
        "! mkdir train\n",
        "! mkdir working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1FeBkuqAnC",
        "outputId": "f9e067ce-b03f-4929-b670-1c259b8e3760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ag_news_csv/\n",
            "ag_news_csv/train.csv\n",
            "ag_news_csv/test.csv\n",
            "ag_news_csv/classes.txt\n",
            "ag_news_csv/readme.txt\n"
          ]
        }
      ],
      "source": [
        "!tar -xzvf ag_news_csv.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iocvey6rqX2K",
        "outputId": "1293ce59-bb36-4574-b437-51c8ff334cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading glove-global-vectors-for-word-representation.zip to /content\n",
            " 99% 455M/458M [00:15<00:00, 34.6MB/s]\n",
            "100% 458M/458M [00:15<00:00, 30.1MB/s]\n",
            "Archive:  glove-global-vectors-for-word-representation.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d 'rtatman/glove-global-vectors-for-word-representation'\n",
        "!unzip glove-global-vectors-for-word-representation.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neD87Uy_VYFk"
      },
      "source": [
        "data process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnN2Z0GMZgx7",
        "outputId": "3d6b2f62-37d6-4a22-d30c-74ae3928b507"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399999/400000 [00:21<00:00, 18854.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of GloVe vectors is torch.Size([400000, 100])\n"
          ]
        }
      ],
      "source": [
        "from torchtext import vocab\n",
        "glove = vocab.Vectors('glove.6B.100d.txt', 'train/')\n",
        "\n",
        "print(f'Shape of GloVe vectors is {glove.vectors.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1pV0xTL4pIUV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path, max_length_sentences=30, max_length_word=35):\n",
        "        super(MyDataset, self).__init__()\n",
        "\n",
        "        texts, labels = [], []\n",
        "        with open(data_path) as csv_file:\n",
        "            reader = csv.reader(csv_file, quotechar='\"')\n",
        "            for idx, line in enumerate(reader):\n",
        "                text = \"\"\n",
        "                for tx in line[1:]:\n",
        "                    text += tx.lower()\n",
        "                    text += \". \"\n",
        "                label = int(line[0])\n",
        "                texts.append(text)\n",
        "                labels.append(label)\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "        self.max_length_sentences = max_length_sentences\n",
        "        self.max_length_word = max_length_word\n",
        "        self.num_classes = len(set(self.labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        text = self.texts[index]\n",
        "        document_encode = [\n",
        "            [glove[word].numpy() for word in word_tokenize(text=sentences)] for sentences\n",
        "            in\n",
        "            sent_tokenize(text=text)]\n",
        "\n",
        "        for sentences in document_encode:\n",
        "            if len(sentences) < self.max_length_word:\n",
        "                extended_words = [glove['pad'].numpy() for _ in range(self.max_length_word - len(sentences))]\n",
        "                sentences.extend(extended_words)\n",
        "\n",
        "        if len(document_encode) < self.max_length_sentences:\n",
        "            extended_sentences = [[glove['pad'].numpy() for _ in range(self.max_length_word)] for _ in\n",
        "                                  range(self.max_length_sentences - len(document_encode))]\n",
        "            document_encode.extend(extended_sentences)\n",
        "\n",
        "        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n",
        "                          :self.max_length_sentences]\n",
        "\n",
        "        document_encode = np.stack(arrays=document_encode, axis=0)\n",
        "        document_encode += 1\n",
        "\n",
        "        return document_encode, label\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "    #print (test.__getitem__(index=0)[0].shape)\n",
        "    #print (test.__getitem__(index=0)[0])\n",
        "    # print (test.__getitem__(index=0)[1]) # lable in(1,..,4)\n",
        "    # print (test.__len__())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V5qPX6sYbOQq"
      },
      "outputs": [],
      "source": [
        "train_set = MyDataset(data_path=\"ag_news_csv/train.csv\")\n",
        "\n",
        "\n",
        "# class_weights = [0.25, 0.25, 0.25, 0.25]\n",
        "# sampler = WeightedRandomSampler(weights=class_weights, num_samples=len(train_set), replacement=True)\n",
        "# dataloader = DataLoader(train_set, batch_size=32, sampler=sampler)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(train_set, batch_size=32,shuffle=True)\n",
        "\n",
        "\n",
        "val_set = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "val_dataloader = DataLoader(val_set, batch_size=32,shuffle=True)\n",
        "\n",
        "# test_set = MyDataset(data_path=\"ag_news_csv/test.csv\")\n",
        "# test_dataloader = DataLoader(test_set, batch_size=32,shuffle=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1rLE8LeVWfn"
      },
      "source": [
        "build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ni8uV70GWm27"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tsLj8w0evAG8"
      },
      "outputs": [],
      "source": [
        "def get_max_lengths(data_path):\n",
        "    word_length_list = []\n",
        "    sent_length_list = []\n",
        "    with open(data_path) as csv_file:\n",
        "        reader = csv.reader(csv_file, quotechar='\"')\n",
        "        for idx, line in enumerate(reader):\n",
        "            text = \"\"\n",
        "            for tx in line[1:]:\n",
        "                text += tx.lower()\n",
        "                text += \" \"\n",
        "            sent_list = sent_tokenize(text)\n",
        "            sent_length_list.append(len(sent_list))\n",
        "\n",
        "            for sent in sent_list:\n",
        "                word_list = word_tokenize(sent)\n",
        "                word_length_list.append(len(word_list))\n",
        "\n",
        "        sorted_word_length = sorted(word_length_list)\n",
        "        sorted_sent_length = sorted(sent_length_list)\n",
        "\n",
        "    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eN5cJEHlVeb7"
      },
      "outputs": [],
      "source": [
        "class HierAttNet(nn.Module):\n",
        "    def __init__(self, word_hidden_size=50, sent_hidden_size=50, batch_size=32, num_classes=4,\n",
        "                 max_sent_length=30, max_word_length=35):\n",
        "        super(HierAttNet, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.word_hidden_size = word_hidden_size\n",
        "        self.sent_hidden_size = sent_hidden_size\n",
        "        self.max_sent_length = max_sent_length\n",
        "        self.max_word_length = max_word_length\n",
        "        self.word_att_net = WordAttNet(word_hidden_size)\n",
        "        self.sent_att_net = SentAttNet(sent_hidden_size)\n",
        "        self._init_hidden_state()\n",
        "\n",
        "        self.dense = torch.nn.Linear(sent_hidden_size*2,num_classes,bias=True)\n",
        "\n",
        "    def _init_hidden_state(self, last_batch_size=None):\n",
        "\n",
        "        if last_batch_size:\n",
        "            batch_size = last_batch_size\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n",
        "        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n",
        "        if torch.cuda.is_available():\n",
        "            self.word_hidden_state = self.word_hidden_state.cuda()\n",
        "            self.sent_hidden_state = self.sent_hidden_state.cuda()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output_list = []\n",
        "\n",
        "        input = input.permute(1, 0, 2, 3)\n",
        "        for i in input: # với mỗi câu i\n",
        "            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0, 2), self.word_hidden_state)\n",
        "\n",
        "            output_list.append(output)\n",
        "\n",
        "        #output = torch.cat(output_list, 0)\n",
        "\n",
        "        output = torch.stack(output_list, dim=0)\n",
        "\n",
        "\n",
        "        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n",
        "        #32 100\n",
        "\n",
        "        output= self.dense(output)\n",
        "        output=F.softmax(output,dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dW5xbFfTWxVE"
      },
      "outputs": [],
      "source": [
        "class SentAttNet(nn.Module):\n",
        "    def __init__(self,  hidden_size=50):\n",
        "        super(SentAttNet, self).__init__()\n",
        "\n",
        "        self.sent_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 10))\n",
        "        self.sent_bias = nn.Parameter(torch.Tensor(10))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(10, 1))\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(100, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "\n",
        "        self.sent_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "\n",
        "        f_output, h_output = self.gru(input, hidden_state)  # feature output and hidden state output\n",
        "        #32 30 100\n",
        "\n",
        "        output = matrix_mul(f_output, self.sent_weight,  self.context_weight, self.sent_bias)\n",
        "        output = F.softmax(output) # 30 32\n",
        "\n",
        "        # 30 32 1 * 30 32 100\n",
        "        output = element_wise_mul(output.permute(1,0,2),f_output.permute(1,0,2))\n",
        "\n",
        "        return output, h_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ddZvJUzXWtMr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WordAttNet(nn.Module):\n",
        "    def __init__(self,  hidden_size=50):\n",
        "        super(WordAttNet, self).__init__()\n",
        "\n",
        "        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 10))\n",
        "        self.word_bias = nn.Parameter(torch.Tensor(10))\n",
        "        self.context_weight = nn.Parameter(torch.Tensor(10, 1))\n",
        "\n",
        "\n",
        "        self.gru = nn.GRU(100, hidden_size, bidirectional=True)\n",
        "        self._create_weights(mean=0.0, std=0.05)\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "\n",
        "        self.word_weight.data.normal_(mean, std)\n",
        "        self.context_weight.data.normal_(mean, std)\n",
        "\n",
        "    def forward(self, input, hidden_state):\n",
        "\n",
        "\n",
        "        f_output, h_output = self.gru(input, hidden_state)  # feature output and hidden state output\n",
        "        #32 35 100\n",
        "\n",
        "        output = matrix_mul(f_output, self.word_weight,  self.context_weight, self.word_bias)\n",
        "        output = F.softmax(output) # 35 32\n",
        "\n",
        "        # 35 32 1 * 35 32 100\n",
        "        output = element_wise_mul(output.permute(1,0,2),f_output.permute(1,0,2))\n",
        "\n",
        "        return output, h_output\n",
        "\n",
        "def element_wise_mul(input1, input2):\n",
        "\n",
        "    feature_list = []\n",
        "    for feature_1, feature_2 in zip(input1, input2):\n",
        "        #feature_1 = feature_1.squeeze(dim=2)\n",
        "        feature = (feature_1 * feature_2).sum(dim=0)\n",
        "        #print(feature.shape)\n",
        "        feature_list.append(feature)\n",
        "\n",
        "    output = torch.stack(feature_list, dim=0)\n",
        "    return output\n",
        "\n",
        "\n",
        "def matrix_mul(input, weight, context_weight,  bias=False):\n",
        "\n",
        "\n",
        "    #feature = feature.unsqueeze(1)\n",
        "    input = torch.matmul(input, weight)\n",
        "    input = input + bias\n",
        "    input = torch.tanh(input)\n",
        "\n",
        "    input = torch.matmul(input, context_weight)\n",
        "\n",
        "    return input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN3i07Dcmezb"
      },
      "outputs": [],
      "source": [
        "model = HierAttNet()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrD2FWxi9Ly"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wAZkni3FjBKc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "def train_network(network,train_iter,optimizer,loss_fn,epoch_num,device):\n",
        "\n",
        "    epoch_loss = 0 # loss per epoch\n",
        "    epoch_acc = 0 # accuracy per epoch\n",
        "    network.train() # set the model in training mode as it requires gradients calculation and updtion\n",
        "\n",
        "    network.zero_grad() # clear all the calculated grdients from previous step\n",
        "\n",
        "\n",
        "    # turn off while testing using  model.eval() and torch.no_grad() block\n",
        "\n",
        "    for batch in tqdm(train_iter,f\"Epoch: {epoch_num}\"):\n",
        "        batch =[t.to(device) for t in batch]\n",
        "        # data will be shown to model in batches per epoch to calculate gradients per batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        network._init_hidden_state()\n",
        "        predictions = network(batch[0])\n",
        "        one_hot_batch = F.one_hot((batch[1]-1).to(torch.int64), num_classes=4).to(torch.float)\n",
        "        loss = torch.nn.CrossEntropyLoss()(predictions, one_hot_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # predictions = network(batch[0])\n",
        "        # one_hot_batch = F.one_hot((batch[1]-1).to(torch.int64), num_classes=4).to(torch.float)\n",
        "\n",
        "\n",
        "        # loss = loss_fn(predictions,one_hot_batch) # calculate loss on the whole batch\n",
        "\n",
        "\n",
        "        # Tạo một tensor mới với giá trị 1 ở vị trí tương ứng với giá trị lớn nhất và 0 cho các vị trí khác\n",
        "        pred_classes = torch.zeros_like(predictions)\n",
        "        pred_classes[torch.arange(len(predictions)), torch.argmax(predictions, dim=1)] = 1\n",
        "\n",
        "        correct_preds = (pred_classes * one_hot_batch).float()\n",
        "\n",
        "        accuracy = correct_preds.sum()/len(correct_preds)\n",
        "\n",
        "        # # below two are must and should be used only after calculation of Loss by optimizer\n",
        "        # loss.backward() # Start Back Propagation so that model can calculate gradients based on loss\n",
        "        # optimizer.step() # update the weights based on gradient corresponding to each neuron\n",
        "        # optimizer.zero_grad() # clear all the calculated grdients from previous step\n",
        "\n",
        "\n",
        "        epoch_loss += loss.item()  # add the loss for this batch to calculate the loss for whole epoch\n",
        "        epoch_acc += accuracy.item() # .item() tend to give the exact number from the tensor of shape [1,]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        time.sleep(0.001) # for tqdm progess bar\n",
        "\n",
        "    return epoch_loss/len(train_iter), epoch_acc/len(train_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "khx4jqJYjjoE"
      },
      "outputs": [],
      "source": [
        "def evaluate_network(network,val_test_iter,optimizer,loss_fn,device):\n",
        "    '''\n",
        "    evaluate the network using given parameters\n",
        "    args:\n",
        "        network: any Neural Network object\n",
        "        val_test_iter: iterator of validation/test data\n",
        "        optimizer: optimizer for gradients calculation and updation\n",
        "        loss_fn: appropriate loss function\n",
        "    out:\n",
        "        a tuple of (average_loss,average_accuracy) of floating values for the incoming dataset\n",
        "    '''\n",
        "\n",
        "    total_loss = 0  # total loss for the whole incoming data\n",
        "    total_acc = 0 # total accuracy for the whole data\n",
        "\n",
        "    network.eval() # set the model in evaluation mode to not compute gradients and reduce overhead\n",
        "\n",
        "    with torch.no_grad(): # turn of gradients calculation\n",
        "\n",
        "        for batch in val_test_iter:\n",
        "            batch =[t.to(device) for t in batch]\n",
        "\n",
        "            if len(batch[1]) < 32:\n",
        "              network._init_hidden_state(last_batch_size=len(batch[1]))\n",
        "            else:\n",
        "              network._init_hidden_state()\n",
        "\n",
        "            predictions = network(batch[0])\n",
        "            one_hot_batch = F.one_hot((batch[1]-1).to(torch.int64), num_classes=4).to(torch.float)\n",
        "\n",
        "            loss = loss_fn(predictions,one_hot_batch)\n",
        "\n",
        "            pred_classes = torch.zeros_like(predictions)\n",
        "            pred_classes[torch.arange(len(predictions)), torch.argmax(predictions, dim=1)] = 1\n",
        "\n",
        "            correct_preds = (pred_classes * one_hot_batch).float()\n",
        "\n",
        "            accuracy = correct_preds.sum()/len(correct_preds)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += accuracy.item()\n",
        "\n",
        "        return total_loss/len(val_test_iter), total_acc/len(val_test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2hmkXSjuhw"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8urcHRKRjxFk",
        "outputId": "a79bd040-cfdf-48ac-e4fa-59ab4ac48414"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 1:   0%|          | 0/3750 [00:00<?, ?it/s]<ipython-input-14-89cfe766dfeb>:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = F.softmax(output) # 35 32\n",
            "Epoch: 1: 100%|██████████| 3750/3750 [22:44<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "End of Epoch: 1  |  Train Loss: 0.951  |  Val Loss: 0.869  |  Train Acc: 78.60%  |  Val Acc: 87.45%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 2: 100%|██████████| 3750/3750 [22:49<00:00,  2.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "End of Epoch: 2  |  Train Loss: 0.867  |  Val Loss: 0.857  |  Train Acc: 87.44%  |  Val Acc: 88.50%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 3: 100%|██████████| 3750/3750 [23:06<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "End of Epoch: 3  |  Train Loss: 0.856  |  Val Loss: 0.849  |  Train Acc: 88.62%  |  Val Acc: 89.43%\n"
          ]
        }
      ],
      "source": [
        "from tkinter.constants import E\n",
        "\n",
        "\n",
        "network = HierAttNet()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "lr = 3e-4\n",
        "optimizer = torch.optim.Adam(network.parameters(),lr=lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "network.to(device)\n",
        "\n",
        "# optimizer and losses remains the same\n",
        "\n",
        "EPOCH = 3\n",
        "for epoch in range(EPOCH):\n",
        "    train_loss, train_acc = train_network(network,dataloader,optimizer,loss_fn,epoch+1,device)\n",
        "    val_loss,val_acc = evaluate_network(network,val_dataloader,optimizer,loss_fn,device)\n",
        "    tqdm.write(f'''End of Epoch: {epoch+1}  |  Train Loss: {train_loss:.3f}  |  Val Loss: {val_loss:.3f}  |  Train Acc: {train_acc*100:.2f}%  |  Val Acc: {val_acc*100:.2f}%''')\n",
        "\n",
        "\n",
        "torch.save(network.state_dict(), 'model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WC8WmJBjoZcF",
        "outputId": "1eb79392-fa10-4e32-b238-3d2aca69cb08"
      },
      "outputs": [],
      "source": [
        "model = HierAttNet()\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "network.to(device)\n",
        "\n",
        "val_loss,val_acc = evaluate_network(model,val_dataloader,optimizer,loss_fn,device)\n",
        "tqdm.write(f'''End:  |  Test Loss: {val_loss:.3f}  |   Test Acc: {val_acc*100:.2f}%''')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
